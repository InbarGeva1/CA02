{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CA02: This is a email Spam Classifers that uses Naive Bayes supervised machine learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4p_DvtT7sOIr",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#importing all the necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lilly:\n",
    "[insert explanation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jjKF0nIMwz8_",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def make_Dictionary(root_directory):\n",
    "  all_words = []\n",
    "  emailFiles = [os.path.join(root_directory,f) for f in os.listdir(root_directory)]\n",
    "  for emailFile in emailFiles:\n",
    "    with open(emailFile) as email:\n",
    "      for line in email:\n",
    "        words = line.split()\n",
    "        all_words += words\n",
    "  word_count = Counter(all_words)\n",
    "  words_to_remove = list(word_count)\n",
    "\n",
    "  for item in words_to_remove:\n",
    "    if item.isalpha() == False:\n",
    "      del word_count[item]\n",
    "    elif len(item) == 1:\n",
    "      del word_count[item]\n",
    "  word_count = word_count.most_common(3000)\n",
    "  return word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ella's alternative suggestion to shorten the function above:\n",
    "\n",
    "def make_dictionary(root_directory):\n",
    "    all_words = []\n",
    "    email_files = [os.path.join(root_directory, f) for f in os.listdir(root_directory)]\n",
    "    \n",
    "    for email_file in email_files:\n",
    "        with open(email_file) as email:\n",
    "            all_words.extend(word for line in email for word in line.split()) #extend method to simplify the process of collecting all words\n",
    "    \n",
    "    word_count = Counter(all_words)\n",
    "    \n",
    "    # Remove non-alphabetic and single-character words\n",
    "    word_count = {word: count for word, count in word_count.items() if word.isalpha() and len(word) > 1}\n",
    "    \n",
    "    # Select the top 3000 words using the sorted function with a lambda key function for sorting by count\n",
    "    word_count = dict(sorted(word_count.items(), key=lambda x: x[1], reverse=True)[:3000])\n",
    "    \n",
    "    return word_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create a function called **extract_features**, that will process a directory of email files and generate a feature matrix and corresponding labels for training a machine learning model. \n",
    "\n",
    "1. We create a list of file paths (files) by joining the email_directory with each file name in the directory.\n",
    "2. We initialize a zero-filled feature matrix called **features_matrix** with dimensions (number of files, 3000).\n",
    "3. We iterate through each file, read the content, and count the occurrences of words from a predefined dictionary. These counts are used to populate the corresponding positions in the feature matrix.\n",
    "4. We set the labels in the **training_labels** array based on whether the email file name starts with \"spmsg\" (indicating spam) or not. The function then returns the feature matrix and training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dmVW5xNlyOFc",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def extract_features(email_directory):\n",
    "  files = [os.path.join(email_directory,fi) for fi in os.listdir(email_directory)]\n",
    "  features_matrix = np.zeros((len(files),3000))\n",
    "  training_labels = np.zeros(len(files))\n",
    "  count = 1;\n",
    "  doc_id = 0;\n",
    "  for fil in files:\n",
    "    with open(fil) as fi:\n",
    "      for i, line in enumerate(fi):\n",
    "        if i ==2:\n",
    "          words = line.split()\n",
    "          for word in words:\n",
    "            word_id = 0\n",
    "            for i, d in enumerate(dictionary):\n",
    "              if d[0] == word:\n",
    "                wordID = i\n",
    "                features_matrix[doc_id,word_id] = words.count(word)\n",
    "      training_labels[doc_id] = 0;\n",
    "      filepathTokens = fil.split('/')\n",
    "      lastToken = filepathTokens[len(filepathTokens)-1]\n",
    "      if lastToken.startswith(\"spmsg\"):\n",
    "        training_labels[doc_id] = 1;\n",
    "        count = count + 1\n",
    "      doc_id = doc_id + 1\n",
    "  return features_matrix, training_labels                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ella's alternative suggestion to shorten function above:\n",
    "\n",
    "def extract_features(email_directory, dictionary):\n",
    "    files = [os.path.join(email_directory, fi) for fi in os.listdir(email_directory)]\n",
    "    features_matrix = np.zeros((len(files), 3000))\n",
    "    training_labels = np.zeros(len(files))\n",
    "    \n",
    "    for doc_id, file_path in enumerate(files):\n",
    "        with open(file_path) as file:\n",
    "            # Extract words from the third line\n",
    "            words = file.readline().split() if len(file.readlines()) > 2 else []\n",
    "\n",
    "        for word in words:\n",
    "            if word in dictionary:\n",
    "                word_id = dictionary[word]\n",
    "                features_matrix[doc_id, word_id] = words.count(word)\n",
    "\n",
    "        # Set the label based on the file name\n",
    "        training_labels[doc_id] = 1 if os.path.basename(file_path).startswith(\"spmsg\") else 0\n",
    "\n",
    "    return features_matrix, training_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zoq-rE7Mx0pp",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Connecting the data by writing the path to the 2 folders\n",
    "TRAIN_DATA = './train-mails'\n",
    "TEST_DATA = './test-mails'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading and processing emails from TRAIN and TEST folders\n"
     ]
    }
   ],
   "source": [
    "#Reading the train and test folders  and processing the email file\n",
    "dictionary = make_Dictionary(TRAIN_DATA)\n",
    "\n",
    "print (\"reading and processing emails from TRAIN and TEST folders\")\n",
    "features_matrix, labels = extract_features(TRAIN_DATA)\n",
    "test_features_matrix, test_labels = extract_features(TEST_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model using Guassian Naibe Bayes algorithm\n"
     ]
    }
   ],
   "source": [
    "# Create Gaussian Naive Bayes classifier\n",
    "gnb = GaussianNB()\n",
    "print (\"Training Model using Guassian Naibe Bayes algorithm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed\n"
     ]
    }
   ],
   "source": [
    "# Train the classifier using the features and labels from the training set\n",
    "gnb.fit(features_matrix, labels)\n",
    "print (\"Training completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing trained model to predict Test Data labels\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
       "       1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1.,\n",
       "       0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1.,\n",
       "       1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
       "       1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
       "       0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
       "       0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0.,\n",
       "       0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
       "       1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
       "       0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0.,\n",
       "       1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0.,\n",
       "       0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
       "       1., 0., 0., 1., 1.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Once we trained the data, we used the model to make predictions on the test set\n",
    "predictions = gnb.predict(test_features_matrix)\n",
    "print (\"testing trained model to predict Test Data labels\")\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Lables with the Test Lables:\n",
      "Accuracy: 0.9653846153846154\n"
     ]
    }
   ],
   "source": [
    "print (\"Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Lables with the Test Lables:\")\n",
    "#Here we evaluating the performance of the model using the metric method accuracy provided by sklearn.metrics module\n",
    "\n",
    "#Calculating accuracy:\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M5_mPrvN586A"
   },
   "source": [
    "======================= END OF PROGRAM ========================="
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOaSi3qlFUlqTup/1esXCKD",
   "collapsed_sections": [],
   "name": "naive_bayes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
